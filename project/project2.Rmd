---
title: "Project2"
author: "Samantha Fuentes"
date: "11/16/2020"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(car)
head(KosteckiDillon)
```

This dataset determines the treatment of migraine headaches and if they work. I got this dataset from the cars package because I am interested in the pharmaceutical companies and how they test if their treatments work. This research was done by Tammy Kostecki-Dillon. The variables are patient ID, time on treatment, days from the start of the study, if the migrane hahs an Aura mix or not, the age when treatment started, the airquality, the medication, and if there was a headache while on the medication, and the sex of the patient. There are 4,152 observations. 

```{r}
man1<-manova(cbind(time,dos, age, airq)~headache, data=KosteckiDillon)
summary(man1)
summary.aov(man1)
KosteckiDillon%>%group_by(headache)%>%summarize(mean(time),mean(dos), mean(age), mean(airq))
pairwise.t.test(KosteckiDillon$time,KosteckiDillon$headache, p.adj="none") 
pairwise.t.test(KosteckiDillon$dos,KosteckiDillon$headache, p.adj="none")
pairwise.t.test(KosteckiDillon$age,KosteckiDillon$headache, p.adj="none")
pairwise.t.test(KosteckiDillon$airq,KosteckiDillon$headache, p.adj="none")
1 - 0.95^9
.05/9
library(rstatix)

group <- KosteckiDillon$headache 
DVs <- KosteckiDillon %>% select(time,dos, age, airq)

#Test multivariate normality for each group (null: assumption met)
sapply(split(DVs,group), mshapiro_test)

#If any p<.05, stop. If not, test homogeneity of covariance matrices

```
The MANOVA test showed there was a significant difference between time, dos, age, and air quality. ANOVA showed there was a significant mean difference across time and age. The post hoc t-tests showed time and age again were significantly different. There were 9 tests performed, thus meaning the type I error was 0.3698. The bonferroni correction was 0.00556. This correction only makes age a significant difference. The Manova test assumptions were most likely not met because when testing the multivariate normality was not met. The p values were each less than .05, meaning this assumption was rejected.

```{r}
h0<- mean(KosteckiDillon[KosteckiDillon$headache=="yes",]$age) -                    
    mean(KosteckiDillon[KosteckiDillon$headache=="no",]$age)
mean_diff<-vector()
for(i in 1:5000){    
  boot <- KosteckiDillon %>% sample_frac(replace = T) 
  mean_diff[i] <- mean(boot[boot$headache=="yes",]$age) -                    
    mean(boot[boot$headache=="no",]$age)
} 
mean(mean_diff)
mean(h0)
ggplot(KosteckiDillon,aes(age,fill=headache))+geom_histogram(bins=6.5)+  facet_wrap(~headache,ncol=2)+theme(legend.position="none")
KosteckiDillon %>% group_by(headache) %>% summarize(m = mean(age)) %>% 
    summarize(diff(m))
mean(mean_diff > 1.37735 | mean_diff < -1.37735)
```
The null hypothesis is that the mean difference for ages is the same for those who feel a headache and those that do not. The alternative hypothesis is that the mean difference for ages are not the same for those that feel headaches and those that do not. The p-value was 0.4862, so we failed to reject the null hothesis. The mean difference of the sample was -1.37735. The mean difference after making a random sample was -1.368408 but according to the p-value this difference was not significant. The p-value was 0.4862.

```{r}
library(tidyverse)
library(lmtest)
library(sandwich)
KosteckiDillon$age_m <- KosteckiDillon$age - mean(KosteckiDillon$age, na.rm = T)
fit<-lm(airq~headache*age_m, data=KosteckiDillon)
summary(fit)
ggplot(KosteckiDillon, aes(x = age_m, y = airq, group = headache)) + 
    geom_point(aes(color = headache)) + geom_smooth(method = "lm", 
    aes(color = headache))
resids<-fit$residuals
fitvals<-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red')
ggplot()+geom_histogram(aes(resids), bins=20)
shapiro.test(resids)
coeftest(fit, vcov = vcovHC(fit))
```
The intercept is the predicted air quality for an average aged person with no headache which is 24.677. Headacheyes is when controlling for age, air quality for those that had a headache was 0.254 higher than those that said they did nto have a headache. As for age_m, it is when there is no headache, there is a decrease of 0.039 oof airquality for every one unit increase in age on average. As for headacheyes*age_m, the slope for age on airquality is 0.0463 higher for those that experience having a headache comared to those that did not have a headache. The linearity and homoskedsaticity seemed normal but the normality did not look normal, so it did not pass the test. When doing robust standard errors, the coefficients did not change. This means that the homoskedacity was met by this model because the robust SEs do not violate homoskedacity. The standard errors did not change much and the p-values showed that the intercept was still the onl significant value. The adjusted r-squared is 0.0002.

```{r}
samp<-replicate(5000,{ 
  boot<-sample_frac(KosteckiDillon,replace=T)    
  fit<-lm(airq~headache*age_m, data=boot)     
  coef(fit) 
  })
samp %>% t %>% as.data.frame %>% summarize_all(sd)
```
When comparing the bootstrap standard errors to the robust standard errors, they are very similar. They differ by around 0.001 in some and a little more in others. The robust standard errors are onl a bit higher than the bootstrapped standard errors. 
```{r}
Kostecki1<-KosteckiDillon%>%mutate(headache2=ifelse(headache=="yes",1,0))
head(Kostecki1)
fit1 <- glm(headache2 ~ medication+age, data=Kostecki1, family=binomial(link="logit"))
summary(fit1)
prob <- predict(fit1, type = "response")
pred <- ifelse(prob > 0.5, 1, 0)
table(prediction = pred, truth = Kostecki1$headache2) %>% addmargins
#accuracy
(414+2356)/4152
#tnr
2356/3428
#tpr
414/724
#ppv
414/1486
Kostecki1$prob <- predict(fit1, newdata = Kostecki1, 
    type = "response")
kostecki2 <- Kostecki1 %>% mutate(headache3 = as.factor(Kostecki1$headache2))
ggplot(kostecki2, aes(prob , fill=headache3)) + 
    geom_density(alpha=.3)
library(plotROC)
ROCplot <- ggplot(Kostecki1) + geom_roc(aes(d = headache2, 
    m = prob), n.cuts = 0)
ROCplot
calc_auc(ROCplot)
```
The intercept is the odds of having a headache for a middleaged person with continuing medication and it is 1.195. The medication none is when controlling for age, the odds of having a headache is 0.877 times less than the odds of having a headache while continuing medication. The medication reduced is when controlling for age, the odds of having a headache is 0.628 times more than the odds of having a headache while continuing the medication. Age is when controlling for medication, the odds of having a headache is 0.013 times less for ages other than those of middleage. The accuracy is 0.6671484. The specificity is 0.6872812. The sensitivity is 0.5718232. The precision is 0.2786003. The AUC is 0.633 which is poor. This is a really bad plot since it is pretty close to a straight line, meaning it is a random classifier. 

```{r}
library(glmnet)
class_diag <- function(probs,truth){
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)  
  acc=sum(diag(tab))/sum(tab)  
  sens=tab[2,2]/colSums(tab)[2]  
  spec=tab[1,1]/colSums(tab)[1] 
  ppv=tab[2,2]/rowSums(tab)[2]  
  f1=2*(sens*ppv)/(sens+ppv)
  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  ord<-order(probs, decreasing=TRUE)  
  probs <- probs[ord]; truth <- truth[ord] 
  TPR=cumsum(truth)/max(1,sum(truth))  
  FPR=cumsum(!truth)/max(1,sum(!truth))  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)  
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)  
  n <- length(TPR)  
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )  
  data.frame(acc,sens,spec,ppv,f1,auc)
}
fit2<-glm(headache2~.,data=Kostecki1,family="binomial")
coef(fit2)
probs<-predict(fit2,type="response") 
class_diag(probs,Kostecki1$headache2) 
k=10
data<-Kostecki1[sample(nrow(Kostecki1)),] 
folds<-cut(seq(1:nrow(Kostecki1)),breaks=k,labels=F) 
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]  
  test<-data[folds==i,]  
  truth<-test$headache2 
  fit<-glm(headache2~.,data=Kostecki1,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  diags<-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)
y<-as.matrix(Kostecki1$headache2)
x<-model.matrix(headache2~.,data=Kostecki1)[,-1] 
cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso)
k=10
data<-Kostecki1[sample(nrow(Kostecki1)),] 
folds<-cut(seq(1:nrow(Kostecki1)),breaks=k,labels=F) 
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]  
  test<-data[folds==i,]  
  truth<-test$headache2 
  fit<-glm(headache2~headache,data=Kostecki1,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  diags<-rbind(diags,class_diag(probs,truth))
}
diags%>%summarize_all(mean)
```
The accuracy, sensitivity, specificity, precision, and AUC are all 1. This either means that they were all really great and the model was perfect or there was an error when computing this. After doing a 10-fold CV with the same sample, I got the accuracy of 0.975, sensitivity of 0.769, specificity of 0.993, pprecision of 0.861, and AUC of 0.983. This is still a really good model and seems more accurate than a perfect score from earlier. Even though the AUC decreased from the in-sample model, it is still really high and is considered great. When doing lasso, only the intercept adn headacheyes were retained. This means that headache yes is the most predictive variable. The AUC after only doing testing the variables on the lasso showed was great at 0.989. This was a little higher than the previous but was still less than the first one, which had an AUC of 1. Overall, I feel this is the most accurate since it is not perfect but is the highest. 


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
